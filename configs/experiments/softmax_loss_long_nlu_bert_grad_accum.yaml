experiment_name: softmax_loss_long_nlu_bert_grad_accum

model_name: 'DeepPavlov/rubert-base-cased-sentence'

backbone_model: BertModel
freeze_backbone: true
main_model: SiameseNetwork
cos_margin: 0.3

model:
  encoder_hidden: 768
  output_dim: 256
  n_chunks: 4
  chunk_size: 512

criterion_name: SoftmaxLoss

optimizer:
  lr: 5e-5
#  weight_decay: 1e-4

#scheduler:
#  num_warmup_steps: 200

datasets:
  news_pad_len: 512
  broadcast_pad_len: 2048

loaders:
  batch_size: 350 # for 16gb Tesla P100

trainer:
  max_epochs: 3
  accumulate_grad_batches: 2