experiment_name: softmax_loss_long_nlu_bert_grad_accum

model_name: 'DeepPavlov/rubert-base-cased-sentence'

backbone_model: BertModel
freeze_backbone: true
main_model: SiameseNetwork
model_output_prob: false
cos_margin: 0.7

model:
  encoder_hidden: 768
  output_dim: 256
  n_chunks: 4
  chunk_size: 512

criterion_name: SoftmaxLoss

criterion:
  norm_vectors: false

optimizer:
  lr: 5e-5
#  weight_decay: 1e-4

#scheduler:
#  num_warmup_steps: 200

datasets:
  news_pad_len: 512
  broadcast_pad_len: 2048

loaders:
  batch_size: 350 # for 16gb Tesla P100

trainer:
  max_epochs: 6
  accumulate_grad_batches: 2