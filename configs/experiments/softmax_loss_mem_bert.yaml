experiment_name: softmax_loss_mem_bert_grad_accum

backbone_model: BertModel
freeze_backbone: true
main_model: SiameseMemoryNetwork
cos_margin: 0.25

model:
  encoder_hidden: 768
  output_dim: 256
  n_chunks: 4
  chunk_size: 512
  attention_hidden: 128
  n_heads: 4
  update_coeffs: 0.75 0.25
  input_size: 768
  output_size: 768
  mem_elements: 16
  k: 4

criterion_name: SoftmaxLoss

optimizer:
  lr: 5e-5
#  weight_decay: 1e-4

#scheduler:
#  num_warmup_steps: 200

datasets:
  news_pad_len: 512
  broadcast_pad_len: 2048

loaders:
  batch_size: 233

trainer:
  accumulate_grad_batches: 3
  max_epochs: 30